<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="just a record of programmer's life"><title>[读Paper]information-extraction-by-acquiring-external-evidence-with-reinforcement-learning | 墨写</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">[读Paper]information-extraction-by-acquiring-external-evidence-with-reinforcement-learning</h1><a id="logo" href="/.">墨写</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">[读Paper]information-extraction-by-acquiring-external-evidence-with-reinforcement-learning</h1><div class="post-meta">Nov 22, 2016<script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#简介"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型框架"><span class="toc-number">2.</span> <span class="toc-text">模型框架</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#State"><span class="toc-number">2.1.</span> <span class="toc-text">State</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Action"><span class="toc-number">2.2.</span> <span class="toc-text">Action</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Rewards"><span class="toc-number">2.3.</span> <span class="toc-text">Rewards</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#强化学习"><span class="toc-number">3.</span> <span class="toc-text">强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#方法对比"><span class="toc-number">4.</span> <span class="toc-text">方法对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参考"><span class="toc-number">6.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="post-content"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>利用自然语言处理技术从文档中来提取或者补全实体信息是非常有用的，然而也充满挑战。文档可能并不含有直观的信息，如下图，我们想从一个关于枪杀案的新闻中提取出枪手、受害者人数的信息，其中，受害者人数无法直接提取，需要复杂的方法才能够得出。</p>
<p><img src="/images/rl_example1.png" alt=""></p>
<p>于是，论文提出了引用外部数据，并利用强化学习框架来解决这个问题。 对原文档构建query，再利用搜索引擎，我们就可以得到多个相关的文档。同样上述的例子，在下图两个新闻文档中，就可以分别都很简单地提取出受害者人数和枪手的名字这两个信息，系统对这些信息进行整合，就可以提取出所有的信息。</p>
<p><img src="/images/rl_example2.png" alt=""></p>
<h3 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h3><p><img src="/images/rl_framework.png" alt=""></p>
<p>整个信息抽取的过程是一个马尔科夫决策过程(<strong>MDP</strong>), 每一步，系统将从新文档中提取出的信息($e_new$)和现有的信息($e_cur$)进行整合(<strong>Reconcile</strong>)，并决定如何选取下一个query来进行搜索并提取更多的文章(<strong>Q</strong>)。</p>
<h4 id="State"><a href="#State" class="headerlink" title="State"></a>State</h4><p>其中，对于每一个状态(<strong>State</strong>)具体情况，包含有：</p>
<ul>
<li>$e_cur$的confidence score</li>
<li>$e_new$的confidence score,*   $e_cur$ 和 $e_new$的匹配情况</li>
<li>contaxt words的unigram/tf-idf值</li>
<li>原文档和新文档的tf-idf相似度</li>
</ul>
<p><img src="/images/rl_state.png" alt=""></p>
<h4 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h4><p>一个动作(<strong>Action</strong>)包含整合与查询新文档，即$a = (d, q)$。 其中，d包括接受部分实体的信息，接受全部实体的信息，拒绝全部实体的信息，停止四种类型的动作。 转移方程$T(s’|s, a$, 根据现在状态S和动作a即可以获得下一个状态。而由现有状态来选择执行什么动作a则是由Agent决定的，这也是使用强化学习训练的地方。</p>
<h4 id="Rewards"><a href="#Rewards" class="headerlink" title="Rewards"></a>Rewards</h4><p>对于强化学习，我们还需要定义Rewards函数，用来最大化最后的抽取accuracy并且惩罚过多的query动作。所有实体现有状态的accuracy值和之前状accuracy值之差的和即为reward值。<br><img src="/images/rl_reward.png" alt=""></p>
<p>下面是框架的伪代码。其中，13行抽取实体是用的基于最大熵的模型，会在第四节进行介绍。<br><img src="/images/rl_code1.png" alt=""></p>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>Agent 定义了一个state-action 函数 $Q(s, a)$ 来决定在state s下该执行那个action a。常用Q-learning对最优值进行求解，即迭代地利用得到的rewards对$Q(s, a)$ 进行更新。论文使用了一个深度Q-network($DQN$)对该函数进行学习，下图即为其结构图。</p>
<p><img src="/images/rl_arch.png" alt=""></p>
<p>输入为状态向量s, 分别输出整合动作d和查询动作q。参数的学习利用了RMSprop梯度下降法,每次参数迭代旨在减少DQN预测的$Q(s_t,a_t;θ)$ 和有Bellman equation得到的值$r_t+ \gama max_a Q(s_(t+1),a;θ)$的差距。这些都是强化学习里面的概念，不展开讲了。下面是训练Agent的伪代码。</p>
<p><img src="/images/rl_code2.png" alt=""></p>
<h3 id="方法对比"><a href="#方法对比" class="headerlink" title="方法对比"></a>方法对比</h3><p>论文将强化学习模型与其他方法进行对比。其中，其他模型有：</p>
<ul>
<li>基于CRF和最大熵（Maxent）的分类模型，它们用到的特征如下：</li>
</ul>
<p><img src="/images/rl_feature.png" alt=""></p>
<p>其中，Maxent也在该强化学习模型里用来进行实体抽取。</p>
<ul>
<li>对多个文档的抽取信息进行融合的模型，分别有基于打分的Confidence和基于次数的Majority模型</li>
<li>输入和DQN一样的meta-classifer.</li>
</ul>
<p>论文实验的数据集有两个</p>
<ul>
<li>Gun Violence archive，是关于枪杀案的新闻存档，需要提取的信息有:1 枪手的名字, 2 遇害者人数, 3 受伤人数, 发生的城市。</li>
<li>Foodshield EMA, 包含了食物掺假事件的报道，需要提取的信息有: 1 受影响的食物 2 掺杂物，3 事件发生的地址。</li>
</ul>
<p>下面是在这两个数据集中增强学习的性能，从 accuracy可以看出它比其他模型都要好。<br><img src="/images/rl_result.png" alt=""></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>论文将增强学习运用到信息抽取上，利用了不同文本对同一实体信息抽取难度的不同，协调各个源的偏差，有效的提高了抽取的准确度。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://arxiv.org/pdf/1603.07954.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.07954.pdf</a></p>
</div><div class="tags"><a href="/tags/NLP/">NLP</a><a href="/tags/Reinforcement-learning/">Reinforcement learning</a><a href="/tags/IR/">IR</a></div><div class="post-nav"><a class="pre" href="/2017/08/28/scala-notes/">scala notes</a><a class="next" href="/2016/09/10/Libnids抓不到包/">Libnids抓不到包</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/ESL/" style="font-size: 15px;">ESL</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/programming/" style="font-size: 15px;">programming</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/摄影/" style="font-size: 15px;">摄影</a> <a href="/tags/ML/" style="font-size: 15px;">ML</a> <a href="/tags/plot/" style="font-size: 15px;">plot</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/Reinforcement-learning/" style="font-size: 15px;">Reinforcement learning</a> <a href="/tags/IR/" style="font-size: 15px;">IR</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/Reading/" style="font-size: 15px;">Reading</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/photograph-canon/">photograph-canon</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/22/linux-tips/">linux_tips</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/10/ESL-chap2-notes/">ESL_chap2_notes</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/28/scala-notes/">scala notes</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/22/读Paper-information-extraction-by-acquiring-external-evidence-with-reinforcement-learning/">[读Paper]information-extraction-by-acquiring-external-evidence-with-reinforcement-learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/10/Libnids抓不到包/">Libnids抓不到包</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/08/25/effective-c-notes/">effective c++ notes</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/24/《百年孤独》家谱/">《百年孤独》家谱</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/25/matplotlib/">matplotlib</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">墨写.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>