<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="just a record of programmer's life"><title>Paper|sentence_ordering_and_coherence_modeling | 墨写</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Paper|sentence_ordering_and_coherence_modeling</h1><a id="logo" href="/.">墨写</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Paper|sentence_ordering_and_coherence_modeling</h1><div class="post-meta">May 19, 2018<span> | </span><span class="category"><a href="/categories/nlp/">nlp</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#摘要"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#问题"><span class="toc-number">2.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型"><span class="toc-number">3.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实验和扩展"><span class="toc-number">4.</span> <span class="toc-text">实验和扩展</span></a></li></ol></div></div><div class="post-content"><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>如何理解文本的连贯性和句子间的内在结构联系，对于<strong>多文本摘要</strong>、<strong>QA</strong>等NLP问题来说十分重要。这篇文章基于set-to-sequence框架，提出了一种RNN-base的端到端的深度学习方法，来解决句子的排序和连贯性问题。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol>
<li>给一个文档和它打乱的句子集合， 模型对句子进行排序。</li>
<li>给一个文档和他打乱的句子集合， 模型选取句子作为文档的摘要。这篇文章选取的是科学文章，它的摘要比较规范。</li>
</ol>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>模型模仿了人类解决这个问题的思维，即先读取句子理解它们的意思，排序时再一个一个选择句子。主要分为<em>read</em>， <em>process</em> 和<em>write</em>三个部分。</p>
<p>我们首先来看下模型的框架, 主要由一个 sentence encoder RNN， set encoder RNN和decoder RNN组成。<br><img src="/images/sentence_ordering_framework.png" alt="model framework"></p>
<p><strong><code>read</code></strong><br>sentence encoder RNN没什么好讲的，就是一个LSTM网络把各个句子给向量化，得到${s_1, s_2, …, s_n}$， 它们被用作为<em>sentence memeory</em></p>
<p><strong><code>process</code></strong><br>就是图中的set encoder部分。LSTM网络每一步的输入是前一步的hidden state与memory进行<strong>attention</strong>得到。set encoder会执行m步，称为<em>read cycles</em>, 注意到这个encoder的结果与${s_1, s_2, …, s_n}$的顺序是不相干的。<br><img src="/images/sentence_ordering_encoder.png" alt="set encoder"></p>
<p><strong><code>write</code></strong><br>decoder是一个<em>pointer network</em>, 在训练时，正确顺序的句子$(s_{o_1}, s_{o_2},…,s_{o_n}) = (x^1, x^2,…,x^n)$ 作为LSTM的输入。预测时前一步的输出$\hat x^{t-1}$作为当前的输入。这里$e^{t,i}_{dec}$ 代表的是在位置$t$，$s_i$作为正确句子的概率。$f$是score函数，可以选择一层feed-forward网络等。<br><img src="/images/sentence_ordering_decoder.png" alt="decoder"></p>
<p><strong><code>contrastive sentences</code></strong><br>为了增加鲁棒性，加入一些随机句子。</p>
<p><strong><code>coherence modeling</code> and <code>training objective</code></strong><br>对于一个结果$(s_{p1}, s_{p2},…,s_{pk}$, coherence score的定义如下:<br><img src="/images/sentence_ordering_coherence.png" alt="coherence score"></p>
<p>故最大似然的目标函数是<br><img src="/images/sentence_ordering_objective.png" alt="objective"></p>
<h2 id="实验和扩展"><a href="#实验和扩展" class="headerlink" title="实验和扩展"></a>实验和扩展</h2><p><em>order descrimination</em><br>即给定一个paragraph和它打乱重组的版本集合，选出最相近的，这个问题挑战性并不大。</p>
<p><em>sentence ordering</em><br>给定一堆句子，给它们排序组合成一个段落。主要选取的是科技论文的摘要，因为它们的逻辑结构比较清晰。</p>
<p><em>sentence ordering and summarization</em><br>可以把模型应用于文档摘要，输入文章的句子，然后按照顺序挑选出句子直到预测出结束符。由于这里不要求输入句子的顺序，故也适用于多文档摘要。可以用事先在ordering task上训练好模型，然后将其用于改善摘要模型。而ordering task可以用大量的<em>未标注语料</em>实现，这点是相当有价值的。</p>
<p><em>learned sentence representations</em><br>训练好的sentence encoder 可以用来做句子的向量表达，提升其它任务的表现。</p>
</div><div class="tags"><a href="/tags/NLP/">NLP</a><a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/Summarization/">Summarization</a></div><div class="post-nav"><a class="next" href="/2018/04/01/reusing-existing-browser-session-selenium/">selenium-如何实现重复使用已经打开的浏览器</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/life/">life</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/nlp/">nlp</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/program/">program</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/selenium/" style="font-size: 15px;">selenium</a> <a href="/tags/ESL/" style="font-size: 15px;">ESL</a> <a href="/tags/programming/" style="font-size: 15px;">programming</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/plot/" style="font-size: 15px;">plot</a> <a href="/tags/摄影/" style="font-size: 15px;">摄影</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/ML/" style="font-size: 15px;">ML</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Summarization/" style="font-size: 15px;">Summarization</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/Reading/" style="font-size: 15px;">Reading</a> <a href="/tags/Reinforcement-learning/" style="font-size: 15px;">Reinforcement learning</a> <a href="/tags/IR/" style="font-size: 15px;">IR</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/05/19/sentence-ordering-and-coherence-modeling/">Paper|sentence_ordering_and_coherence_modeling</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/reusing-existing-browser-session-selenium/">selenium-如何实现重复使用已经打开的浏览器</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/27/photograph-canon/">photograph-canon</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/22/linux-tips/">linux_tips</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/10/ESL-chap2-notes/">ESL_chap2_notes</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/28/scala-notes/">scala notes</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/22/读Paper-information-extraction-by-acquiring-external-evidence-with-reinforcement-learning/">[读Paper]information-extraction-by-acquiring-external-evidence-with-reinforcement-learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/10/Libnids抓不到包/">Libnids抓不到包</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/08/25/effective-c-notes/">effective c++ notes</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/24/《百年孤独》家谱/">《百年孤独》家谱</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">墨写.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>